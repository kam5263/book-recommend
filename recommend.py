import pandas as pd
from scipy import sparse
import pickle
from sklearn.metrics.pairwise import cosine_similarity
from konlpy.tag import Okt
from typing import List

# ë¶ˆëŸ¬ì˜¤ê¸°ë§Œ í•´ë„ ì¶”ì²œ ê°€ëŠ¥
df = pd.read_csv("preprocessed_books.csv", encoding="utf-8-sig")
tfidf_matrix = sparse.load_npz("tfidf_matrix.npz")

# vectorizerë„ ë¶ˆëŸ¬ì™€ì•¼ ìƒˆë¡œìš´ ë¬¸ì¥ì„ ì¶”ì²œí•˜ë ¤ë©´ ì‚¬ìš© ê°€ëŠ¥
with open("tfidf_vectorizer.pkl", "rb") as f:
    vectorizer = pickle.load(f)

def handle_q8_sijip(item) -> List[str]:
    if item.answer == "ì‹œì§‘_ìì£¼":
        return ["ì‹œì§‘"] * 3
    elif item.answer == "ì‹œì§‘_ê°€ë”":
        return ["ì‹œì§‘"]
    return []

NEGATIVE_KEYWORDS = {"ê±°ì˜", "ì•ˆ", "ì—†ë‹¤", "ì‹«ë‹¤", "ì‹«ì–´", "ì•„ë‹ˆë‹¤"}
# Q5 ì‘ë‹µ â†’ ê°•ì¡°í•  ì§ˆë¬¸ ID ë° ê°€ì¤‘ì¹˜ ëª©ë¡
Q5_WEIGHT_MAP = {
    "ì‘ê°€": [(2, 2.0)],
    "ë‚´ìš©": [(3, 1.0), (4, 2.0), (7, 3.0), (9, 4.0)],
    "ê°ì„±": [(1, 2.0), (6, 3.0)],
    "ì¶”ì²œ": [(10, 2.0)]
}

SPECIAL_HANDLERS = {
    8: handle_q8_sijip
    # í–¥í›„ ì¶”ê°€ ê°€ëŠ¥
}

def preprocess_korean_text(text):
    okt = Okt()
    if pd.isna(text):
        return ""
    tokens = okt.pos(text, stem=True)
    filtered = [word for word, tag in tokens if tag in ['Noun', 'Adjective'] and word not in NEGATIVE_KEYWORDS]
    return ' '.join(filtered)

def recommend_boods_by_input(user_input, top_n=5):
    # ì‚¬ìš©ìì˜ ì„ íƒì„ í•˜ë‚˜ì˜ ë¬¸ì¥ì²˜ëŸ¼ ë¬¶ìŒ
    user_text = " ".join(user_input)

    # ì´ í…ìŠ¤íŠ¸ë¥¼ í˜•íƒœì†Œ ë¶„ì„í•˜ê³  TF-IDF ë²¡í„°í™”
    user_vec = vectorizer.transform([preprocess_korean_text(user_text)])
    # ì „ì²´ ì±… ë²¡í„°ë“¤ê³¼ ìœ ì‚¬ë„ ë¹„êµ
    similarities = cosine_similarity(user_vec, tfidf_matrix).flatten()
    top_indices = similarities.argsort()[::-1][:top_n+1]

    # ê²°ê³¼ ì¶œë ¥
    return df.iloc[top_indices][["title", "author", "hashtag"]]

def recommend_books_with_reason(user_input: List[dict], top_n=5):
    # Q5 ì‘ë‹µ í™•ì¸
    q5_answer = next((item.answer for item in user_input if item.question_id == 5), None)
    weight_map = Q5_WEIGHT_MAP.get(q5_answer, [])
    # ì§ˆë¬¸ IDë³„ ê°€ì¤‘ì¹˜ dict ìƒì„± (ì—†ìœ¼ë©´ 1.0)
    question_weights = {qid: weight for qid, weight in weight_map}

    answer_texts = []
    for item in user_input:
        # Q8 ì‹œì§‘ ì²˜ë¦¬
        # ğŸ§  íŠ¹ìˆ˜ ë¡œì§ì´ ìˆìœ¼ë©´ ìš°ì„  ì‹¤í–‰
        if item.question_id in SPECIAL_HANDLERS:
            special_keywords = SPECIAL_HANDLERS[item.question_id](item)
            answer_texts.extend(special_keywords)
            continue  # íŠ¹ìˆ˜ ë¡œì§ ì²˜ë¦¬í–ˆìœ¼ë‹ˆ ì¼ë°˜ ë¡œì§ì€ ìŠ¤í‚µ
        
        weight = question_weights.get(item.question_id, 1.0)
        repeat_count = round(weight * 1.0)  # â†’ 2.5 â†’ 2ë²ˆ ë°˜ë³µ (ì •ìˆ˜ë¡œ)
        answer_texts.extend([item.answer] * repeat_count)

    user_text = " ".join(answer_texts)

    # "answer" í•„ë“œë§Œ ì¶”ì¶œí•˜ì—¬ ë²¡í„°í™”
    answer_texts = [item.answer for item in user_input]

    user_text = " ".join(answer_texts)
    user_proc = preprocess_korean_text(user_text)
    user_vec = vectorizer.transform([user_proc])
    similarities = cosine_similarity(user_vec, tfidf_matrix).flatten()
    top_indices = similarities.argsort()[::-1][:top_n]

    feature_names = vectorizer.get_feature_names_out()
    user_vec_array = user_vec.toarray()[0]

    results = []

    for idx in top_indices:
        book = df.iloc[idx]
        book_vec = tfidf_matrix[idx].toarray()[0]

        # ì‚¬ìš©ìì™€ ì±… ëª¨ë‘ì—ì„œ TF-IDFê°€ ë†’ì€ ë‹¨ì–´ ì¶”ì¶œ
        top_user_keywords = set(
            [feature_names[i] for i in user_vec_array.argsort()[::-1][:20] if user_vec_array[i] > 0]
        )
        top_book_keywords = set(
            [feature_names[i] for i in book_vec.argsort()[::-1][:20] if book_vec[i] > 0]
        )

        overlap = list(top_user_keywords & top_book_keywords)
        overlap = sorted(overlap, key=lambda w: user_vec_array[feature_names.tolist().index(w)], reverse=True)
        reason_keywords = ", ".join(overlap[:3]) if overlap else "ê³µí†µëœ í‚¤ì›Œë“œ ì—†ìŒ"

        results.append({
            "title": book["title"],
            "author": book["author"],
            "hashtag": book.get("hashtag", ""),
            "reason": f"'{reason_keywords}' í‚¤ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ì²œë˜ì—ˆìŠµë‹ˆë‹¤."
        })

    return results