import pandas as pd
from scipy import sparse
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from kiwipiepy import Kiwi
from typing import List
import os
import time

BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” (ì „ì—­ì—ì„œ í•œ ë²ˆë§Œ)
kiwi = Kiwi()

print("[DEBUG] csv íŒŒì¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘")
df = pd.read_csv(os.path.join(BASE_DIR, "hangle_preprocessed_books_deduple.csv"), encoding="utf-8-sig")

NEGATIVE_KEYWORDS = {"ê±°ì˜", "ì•ˆ", "ì—†ë‹¤", "ì‹«ë‹¤", "ì‹«ì–´", "ì•„ë‹ˆë‹¤"}
# í˜•íƒœì†Œ ë¶„ì„ê¸° (Kiwi) ê¸°ë°˜ ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_korean_text(text):
    if pd.isna(text):
        return ""
    tokens = kiwi.tokenize(text)
    filtered = [token.form for token in tokens
                if token.tag in ['NNG', 'NNP', 'VA'] and token.form not in NEGATIVE_KEYWORDS]
    return ' '.join(filtered)

# start_time = time.time()
# print("[DEBUG] clean_text ì¬ìƒì„± ì¤‘")
# df["clean_text"] = df["clean_text"].apply(preprocess_korean_text)
# end_time = time.time()
# print(f"ì‹¤í–‰ ì‹œê°„: {end_time - start_time:.4f}ì´ˆ")

start_time = time.time()
# TF-IDF ë²¡í„°í™” (ì§ì ‘ ìƒì„±)
print("[DEBUG] TfidfVectorizer ì‹¤í–‰ ì¤‘")
vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1, 2))
tfidf_matrix = vectorizer.fit_transform(df["clean_text"])
end_time = time.time()
print(f"ì‹¤í–‰ ì‹œê°„: {end_time - start_time:.4f}ì´ˆ")

# íŠ¹ìˆ˜ í•¸ë“¤ëŸ¬ ë° í‚¤ì›Œë“œ ì„¤ì •
def handle_q8_sijip(item) -> List[str]:
    if item.answer == "ì‹œì§‘_ìì£¼":
        return ["ì‹œì§‘"] * 3
    elif item.answer == "ì‹œì§‘_ê°€ë”":
        return ["ì‹œì§‘"]
    return []

# Q5 ì‘ë‹µ â†’ ê°•ì¡°í•  ì§ˆë¬¸ ID ë° ê°€ì¤‘ì¹˜ ëª©ë¡
Q5_WEIGHT_MAP = {
    "ì‘ê°€": [(2, 2.0)],
    "ë‚´ìš©": [(3, 1.0), (4, 2.0), (7, 3.0), (9, 4.0)],
    "ê°ì„±": [(1, 2.0), (6, 3.0)],
    "ì¶”ì²œ": [(10, 2.0)]
}

SPECIAL_HANDLERS = {
    8: handle_q8_sijip
    # í–¥í›„ ì¶”ê°€ ê°€ëŠ¥
}

def recommend_books_with_reason(user_input: List[dict], top_n=5):
    print("[DEBUG] ì¶”ì²œ ìš”ì²­ ì‹œì‘", flush=True)
    print("[DEBUG] ì‚¬ìš©ì ì…ë ¥:", user_input, flush=True)
    # Q5 ì‘ë‹µ í™•ì¸
    q5_answer = next((item.answer for item in user_input if item.question_id == 5), None)
    weight_map = Q5_WEIGHT_MAP.get(q5_answer, [])
    # ì§ˆë¬¸ IDë³„ ê°€ì¤‘ì¹˜ dict ìƒì„± (ì—†ìœ¼ë©´ 1.0)
    question_weights = {qid: weight for qid, weight in weight_map}

    answer_texts = []
    for item in user_input:
        # Q8 ì‹œì§‘ ì²˜ë¦¬
        # ğŸ§  íŠ¹ìˆ˜ ë¡œì§ì´ ìˆìœ¼ë©´ ìš°ì„  ì‹¤í–‰
        if item.question_id in SPECIAL_HANDLERS:
            special_keywords = SPECIAL_HANDLERS[item.question_id](item)
            answer_texts.extend(special_keywords)
            continue  # íŠ¹ìˆ˜ ë¡œì§ ì²˜ë¦¬í–ˆìœ¼ë‹ˆ ì¼ë°˜ ë¡œì§ì€ ìŠ¤í‚µ
        
        weight = question_weights.get(item.question_id, 1.0)
        repeat_count = round(weight * 1.0)  # â†’ 2.5 â†’ 2ë²ˆ ë°˜ë³µ (ì •ìˆ˜ë¡œ)
        answer_texts.extend([item.answer] * repeat_count)

    user_text = " ".join(answer_texts)

    # "answer" í•„ë“œë§Œ ì¶”ì¶œí•˜ì—¬ ë²¡í„°í™”
    answer_texts = [item.answer for item in user_input]

    user_text = " ".join(answer_texts)
    user_proc = preprocess_korean_text(user_text)
    user_vec = vectorizer.transform([user_proc])
    similarities = cosine_similarity(user_vec, tfidf_matrix).flatten()
    top_indices = similarities.argsort()[::-1][:top_n]

    feature_names = vectorizer.get_feature_names_out()
    user_vec_array = user_vec.toarray()[0]

    results = []

    for idx in top_indices:
        book = df.iloc[idx]
        book_vec = tfidf_matrix[idx].toarray()[0]

        # ì‚¬ìš©ìì™€ ì±… ëª¨ë‘ì—ì„œ TF-IDFê°€ ë†’ì€ ë‹¨ì–´ ì¶”ì¶œ
        top_user_keywords = set(
            [feature_names[i] for i in user_vec_array.argsort()[::-1][:20] if user_vec_array[i] > 0]
        )
        top_book_keywords = set(
            [feature_names[i] for i in book_vec.argsort()[::-1][:20] if book_vec[i] > 0]
        )

        overlap = list(top_user_keywords & top_book_keywords)
        overlap = sorted(overlap, key=lambda w: user_vec_array[feature_names.tolist().index(w)], reverse=True)
        reason_keywords = ", ".join(overlap[:3]) if overlap else "ê³µí†µëœ í‚¤ì›Œë“œ ì—†ìŒ"

        results.append({
            "title": book["title"],
            "author": book["author"],
            "hashtag": book.get("hashtag", ""),
            "reason": f"'{reason_keywords}' í‚¤ì›Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ì²œë˜ì—ˆìŠµë‹ˆë‹¤."
        })

    return results